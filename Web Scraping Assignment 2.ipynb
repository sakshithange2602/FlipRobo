{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "except NoSuchElementException as e:\n",
    "    title_tags=' '\n",
    "\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "\n",
    "try:\n",
    "    location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    location_tags=' '\n",
    "    \n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "try:\n",
    "    subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "except NoSuchElementException as e:\n",
    "    subtitle_tags=' '\n",
    "\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "try:\n",
    "    experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    experience_tags=' '\n",
    "    \n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "except NoSuchElementException as e:\n",
    "    title_tags=' '\n",
    "\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "try:\n",
    "    location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    location_tags=' '\n",
    "    \n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "try:\n",
    "    subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "except NoSuchElementException as e:\n",
    "    subtitle_tags=' '\n",
    "\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "try:\n",
    "    experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    experience_tags=' '\n",
    "\n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Please note that you have to scrape full job description. For that you may have to open each job separately as shown below\n",
    "You will get the search result as shown in the below webpage:\n",
    "You have to click on each job title. Here in this case if you will click on “ Data Scientist, Machine Learning” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/data-scientist-jobs-in-banglore-bengaluru'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_description=[]\n",
    "\n",
    "for i in urls[:10]:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        job=driver.find_element_by_xpath(\"//h1[@class='jd-header-title']\")\n",
    "        job_title.append(job.text)\n",
    "    except NoSuchElementException as e:\n",
    "        job_title.append('-')\n",
    "        \n",
    "    try:\n",
    "        desc=driver.find_element_by_xpath(\"//section[@class='job-desc']\")\n",
    "        job_description.append(desc.text)\n",
    "    except NoSuchElementException as e:\n",
    "        job_description.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job_description']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[2]/div[2]/div[2]/label/p\")\n",
    "loc.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sal=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/p\")\n",
    "Sal.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "except NoSuchElementException as e:\n",
    "    title_tags=' '\n",
    "    \n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "\n",
    "try:\n",
    "    location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    location_tags=' '\n",
    "    \n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "try:\n",
    "    subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "except NoSuchElementException as e:\n",
    "    subtitle_tags=' '\n",
    "    \n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "try:\n",
    "    experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    experience_tags=' '\n",
    "    \n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/index.htm'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please login on glassdor site before proceeding next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('sc.keyword')\n",
    "search_job.clear()\n",
    "\n",
    "search_job=driver.find_element_by_id('sc.keyword')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "time.sleep(3)\n",
    "\n",
    "search_loc=driver.find_element_by_id('sc.location')\n",
    "search_loc.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('sc.location')\n",
    "search_loc.send_keys(\"Noida (India)\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    companyname_tags=driver.find_elements_by_xpath(\"//a[@class=' css-l2wjgv e1n63ojh0 jobLink']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    companyname_tags=' '\n",
    "    \n",
    "company_names=[]\n",
    "for i in companyname_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "try:\n",
    "    jobposteddays_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "except NoSuchElementException as e:\n",
    "    jobposteddays_tags=' '\n",
    "    \n",
    "JobPosted_Days=[]\n",
    "for i in jobposteddays_tags:\n",
    "    JobPosted_Days.append(i.text)\n",
    "    \n",
    "try:\n",
    "    rating_tags=driver.find_elements_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\")\n",
    "except NoSuchElementException as e:\n",
    "    rating_tags=' '\n",
    "    \n",
    "Company_Rating=[]\n",
    "for i in rating_tags:\n",
    "    Company_Rating.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['company name']=company_names[0:10]\n",
    "jobs['job posted days']=JobPosted_Days[0:10]\n",
    "jobs['company rating']=Company_Rating[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please login on glassdor site before proceeding next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('KeywordSearch')\n",
    "search_job.clear()\n",
    "\n",
    "search_job=driver.find_element_by_id('KeywordSearch')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('LocationSearch')\n",
    "search_loc.clear()\n",
    "\n",
    "search_loc=driver.find_element_by_id('LocationSearch')\n",
    "search_loc.send_keys(\"Noida (India)\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    companyname_tags=driver.find_elements_by_xpath(\"//*[@id='SalariesByCompany']/div[1]/div[4]/div/div[1]/div/div[2]/p[2]\")\n",
    "except NoSuchElementException as e:\n",
    "    companyname_tags=' '\n",
    "    \n",
    "company_names=[]\n",
    "for i in companyname_tags:\n",
    "    company_names.append(i.text)\n",
    "\"\"\n",
    "try:\n",
    "    AvgSalary_tags=driver.find_elements_by_xpath(\"//*[@id='SalariesByCompany']/div[1]/div[4]/div/div[2]/strong\")\n",
    "except NoSuchElementException as e:\n",
    "    AvgSalary_tags=' '\n",
    "    \n",
    "AvgSalary=[]\n",
    "for i in AvgSalary_tags:\n",
    "    AvgSalary.append(i.text)\n",
    "    \n",
    "try:\n",
    "    minsalary_tags=driver.find_elements_by_xpath(\"//*[@id='SalariesByCompany']/div[1]/div[4]/div/div[3]/div/div[2]/span[1]\")\n",
    "except NoSuchElementException as e:\n",
    "    minsalary_tags=' '\n",
    "    \n",
    "MinSalary=[]\n",
    "for i in minsalary_tags:\n",
    "    MinSalary.append(i.text)\n",
    "MinSalary[0:20:2]\n",
    "\n",
    "try:\n",
    "    maxsalary_tags=driver.find_elements_by_xpath(\"//*[@id='SalariesByCompany']/div[1]/div[4]/div/div[3]/div/div[2]/span[2]\")\n",
    "except NoSuchElementException as e:\n",
    "    maxsalary_tags=' '\n",
    "    \n",
    "MaxSalary=[]\n",
    "for i in maxsalary_tags:\n",
    "    MaxSalary.append(i.text)\n",
    "MaxSalary[1:21:2]\n",
    "\n",
    "import pandas as pd\n",
    "Salaries=pd.DataFrame({})\n",
    "Salaries['company name']=company_names[0:10]\n",
    "Salaries['Average Salary']=AvgSalary[0:10]\n",
    "Salaries['Min Salary']=MinSalary[0:10]\n",
    "Salaries['Max Salary']=MaxSalary[0:10]\n",
    "Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "login_pop.click()\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sunglasses\")\n",
    "search_job.submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "    \n",
    "Brand_name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_name.append(i.text)\n",
    "    \n",
    "try:\n",
    "    description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "except NoSuchElementException as e:\n",
    "    description_tags=' '\n",
    "    \n",
    "Product_Description=[]\n",
    "for i in description_tags:\n",
    "    Product_Description.append(i.text)\n",
    "    \n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "    \n",
    "Product_Price=[]\n",
    "for i in price_tags:\n",
    "    Product_Price.append(i.text)\n",
    "    \n",
    "try:\n",
    "    discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    discount_tags=' '\n",
    "    \n",
    "Price_Discount=[]\n",
    "for i in discount_tags:\n",
    "    Price_Discount.append(i.text)\n",
    "    \n",
    "import pandas as pd\n",
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']=Brand_name[0:100]\n",
    "Sunglasses['Description']=Product_Description[0:100]\n",
    "Sunglasses['Price']=Product_Price[0:100]\n",
    "Sunglasses['Discount']=Price_Discount[0:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'    \n",
    "    driver.get(url)\n",
    "    try:\n",
    "        rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    except NoSuchElementException as e:\n",
    "        rating_tags='-'\n",
    "        \n",
    "    Ratings=[]\n",
    "    for i in rating_tags:\n",
    "        Ratings.append(i.text)\n",
    "    \n",
    "    try:\n",
    "        summary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    except NoSuchElementException as e:\n",
    "        summary_tags='-'\n",
    "        \n",
    "    Summary=[]\n",
    "    for i in summary_tags:\n",
    "        Summary.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        FullReview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']//div//div\")\n",
    "    except NoSuchElementException as e:\n",
    "        FullReview_tags='-'\n",
    "        \n",
    "    FullSummary=[]\n",
    "    for i in FullReview_tags:\n",
    "        FullSummary.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "IPhone=pd.DataFrame({})\n",
    "IPhone['Ratings']=Ratings[0:100]\n",
    "IPhone['Summary']=Summary[0:100]\n",
    "IPhone['FullSummary']=FullSummary[0:100]\n",
    "IPhone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "# Here .click function use to tap on desire elements of webpage\n",
    "login_pop.click()\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sneakers\")\n",
    "search_job.submit()\n",
    "\n",
    "\n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    except NoSuchElementException as e:\n",
    "        brand_tags='-'\n",
    "        \n",
    "    BrandTags=[]\n",
    "    for i in brand_tags:\n",
    "        BrandTags.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        ProductDesc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    except NoSuchElementException as e:\n",
    "        ProductDesc_tags='-'\n",
    "        \n",
    "    ProductDesc=[]\n",
    "    for i in ProductDesc_tags:\n",
    "        ProductDesc.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    except NoSuchElementException as e:\n",
    "        Price_tags='-'\n",
    "        \n",
    "    Price=[]\n",
    "    for i in Price_tags:\n",
    "        Price.append(i.text)\n",
    "        \n",
    "    try:\n",
    "        Discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    except NoSuchElementException as e:\n",
    "        Discount_tags='-'\n",
    "        \n",
    "    Discount=[]\n",
    "    for i in Discount_tags:\n",
    "        Discount.append(i.text)\n",
    "        \n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers['Brand']=BrandTags[0:30]\n",
    "sneakers['Shoe Description']=ProductDesc[0:30]\n",
    "sneakers['Price']=Price[0:30]\n",
    "sneakers['Discount']=Discount[0:30]\n",
    "sneakers    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price filter to “Rs. 6649 to Rs. 13099”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color filter to “Black”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "color.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_loc.send_keys(\"Laptop\")\n",
    "\n",
    "search_btn=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu1=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[3]/ul/li[27]/span/a/span\")\n",
    "cpu1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['Rating']=Product_Desc[0:10]\n",
    "laptop['Price']=Prices[0:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filter “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_loc.send_keys(\"Laptop\")\n",
    "\n",
    "search_btn=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu2=driver.find_element_by_xpath(\"//*[@id='p_n_feature_thirteen_browse-bin/16757432031']/span/a/span\")\n",
    "cpu2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['Rating']=Product_Desc[0:10]\n",
    "laptop['Price']=Prices[0:10]\n",
    "laptop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
