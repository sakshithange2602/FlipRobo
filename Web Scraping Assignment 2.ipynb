{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "    \n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "    \n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import selenium.webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "\n",
    "loc=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[2]/div[2]/div[2]/label/p\")\n",
    "loc.click()\n",
    "Sal=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/p\")\n",
    "Sal.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "\n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/index.htm'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('sc.keyword')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('sc.location')\n",
    "search_loc.send_keys(\"Noida (India)\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\")\n",
    "search_btn.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyname_tags=driver.find_elements_by_xpath(\"//a[@class=' css-l2wjgv e1n63ojh0 jobLink']//span\")\n",
    "company_names=[]\n",
    "for i in companyname_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "jobposteddays_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "JobPosted_Days=[]\n",
    "for i in jobposteddays_tags:\n",
    "    JobPosted_Days.append(i.text)\n",
    "    \n",
    "rating_tags=driver.find_elements_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\")\n",
    "Company_Rating=[]\n",
    "for i in rating_tags:\n",
    "    Company_Rating.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['company name']=company_names[0:10]\n",
    "jobs['job posted days']=JobPosted_Days[0:10]\n",
    "jobs['company rating']=Company_Rating[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('KeywordSearch')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('LocationSearch')\n",
    "search_loc.send_keys(\"Noida (India)\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companyname_tags=driver.find_elements_by_xpath(\"//h3[@class='m-0 css-g261rn']//a\")\n",
    "company_names=[]\n",
    "for i in companyname_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "AvgSalary_tags=driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-4 px-lg-0 d-flex align-items-baseline']//h3\")\n",
    "AvgSalary=[]\n",
    "for i in AvgSalary_tags:\n",
    "    AvgSalary.append(i.text)\n",
    "    \n",
    "minsalary_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p\")\n",
    "MinSalary=[]\n",
    "for i in minsalary_tags:\n",
    "    MinSalary.append(i.text)\n",
    "MinSalary[0:20:2]\n",
    "\n",
    "maxsalary_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p\")\n",
    "MaxSalary=[]\n",
    "for i in maxsalary_tags:\n",
    "    MaxSalary.append(i.text)\n",
    "MaxSalary[1:21:2]\n",
    "\n",
    "rating_tags=driver.find_elements_by_xpath(\"//span[@class='m-0 css-kyx745']\")\n",
    "CompanyRating=[]\n",
    "for i in rating_tags:\n",
    "    CompanyRating.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "Salaries=pd.DataFrame({})\n",
    "Salaries['company name']=company_names[0:10]\n",
    "Salaries['Average Salary']=AvgSalary[0:10]\n",
    "Salaries['Min Salary']=MinSalary[0:10]\n",
    "Salaries['Max Salary']=MaxSalary[0:10]\n",
    "Salaries['Company Rating']=CompanyRating[0:10]\n",
    "Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "login_pop.click()\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sunglasses\")\n",
    "search_job.submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "Brand_name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_name.append(i.text)\n",
    "    \n",
    "description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "\n",
    "Product_Description=[]\n",
    "for i in description_tags:\n",
    "    Product_Description.append(i.text)\n",
    "    \n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "Product_Price=[]\n",
    "for i in price_tags:\n",
    "    Product_Price.append(i.text)\n",
    "    \n",
    "discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "Price_Discount=[]\n",
    "for i in discount_tags:\n",
    "    Price_Discount.append(i.text)\n",
    "    \n",
    "import pandas as pd\n",
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']=Brand_name[0:100]\n",
    "Sunglasses['Description']=Product_Description[0:100]\n",
    "Sunglasses['Price']=Product_Price[0:100]\n",
    "Sunglasses['Discount']=Price_Discount[0:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "  \n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'    \n",
    "    driver.get(url)\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    ProductDesc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    Discount_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']\")\n",
    "    for i in range(len(rating_tags)):\n",
    "        element_list.append([rating_tags[i].text, summary_tags[i].text, FullReview_tags[i].text])\n",
    "  \n",
    "with xlsxwriter.Workbook('IphoneReviewResult.xlsx') as workbook:\n",
    "    worksheet = workbook.add_worksheet()\n",
    "  \n",
    "    for row_num, data in enumerate(element_list):\n",
    "        worksheet.write_row(row_num, 0, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "# Here .click function use to tap on desire elements of webpage\n",
    "login_pop.click()\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sneakers\")\n",
    "search_job.submit()\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "  \n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'    \n",
    "    driver.get(url)\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    ProductDesc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    Discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    for i in range(len(rating_tags)):\n",
    "        element_list.append([brand_tags[i].text, ProductDesc_tags[i].text, Price_tags[i].text, Discount_tags[i].text])\n",
    "  \n",
    "with xlsxwriter.Workbook('SneakersResult.xlsx') as workbook:\n",
    "    worksheet = workbook.add_worksheet()\n",
    "  \n",
    "    for row_num, data in enumerate(element_list):\n",
    "        worksheet.write_row(row_num, 0, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price filter to “Rs. 6649 to Rs. 13099”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color filter to “Black”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "\n",
    "color=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "color.click()\n",
    "\n",
    "brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=job_title[0:10]\n",
    "shoes['Shoe Description']=job_location[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import xlsxwriter\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_loc.send_keys(\"Laptop\")\n",
    "\n",
    "search_btn=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu1=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[9]/span/a/span\")\n",
    "cpu1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "desc_tags=driver.find_elements_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[1]/div/span[3]/div[2]/div[2]/div/span/div/div/div[2]/div[2]/div/div[2]/div/span[1]/span/a/i[1]/span\")\n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['Title']=Brand_Name[0:10]\n",
    "laptop['Rating']=Product_Desc[0:10]\n",
    "laptop['Price']=Prices[0:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filter “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import xlsxwriter\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_loc.send_keys(\"Laptop\")\n",
    "\n",
    "search_btn=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu2=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[8]/span/a/div/label/span\")\n",
    "cpu2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "desc_tags=driver.find_elements_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[1]/div/span[3]/div[2]/div[2]/div/span/div/div/div[2]/div[2]/div/div[2]/div/span[1]/span/a/i[1]/span\")\n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['Title']=Brand_Name[0:10]\n",
    "laptop['Rating']=Product_Desc[0:10]\n",
    "laptop['Price']=Prices[0:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
