{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Analyst\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job-title</th>\n",
       "      <th>job-location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Informatica MDM</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Assistant Vice President - MIS &amp; Reporting ( B...</td>\n",
       "      <td>Mumbai, Bangalore/Bengaluru</td>\n",
       "      <td>INTERTRUSTVITEOS CORPORATE AND FUND SERVICES P...</td>\n",
       "      <td>12-18 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Kolkata, Pune, Chennai, Bangalore/Bengaluru, D...</td>\n",
       "      <td>SA Tech Software (I) Pvt. Ltd.</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring For Data Analyst/ MIS Reporting Analyst...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>PHARMACEUTICAL RESEARCH ASSOCIATES INDIA Pvt Ltd</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Consultant-Data Analyst -Bangalore</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Innovsource Services Private Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data analyst - Google Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>H and M Hennes and Mauritz (P) Ltd.</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior/Regular Business Analyst / Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Luxoft</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SW Technologist I- Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Philips India Limited</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job-title  \\\n",
       "0    Data Scientist / Data Analyst -Business Analyst   \n",
       "1                     Data Analyst - Informatica MDM   \n",
       "2  Assistant Vice President - MIS & Reporting ( B...   \n",
       "3                                       Data Analyst   \n",
       "4  Hiring For Data Analyst/ MIS Reporting Analyst...   \n",
       "5                 Consultant-Data Analyst -Bangalore   \n",
       "6                    Data analyst - Google Analytics   \n",
       "7     Senior/Regular Business Analyst / Data Analyst   \n",
       "8                    SW Technologist I- Data Analyst   \n",
       "9                                       Data Analyst   \n",
       "\n",
       "                                        job-location  \\\n",
       "0  Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                        Mumbai, Bangalore/Bengaluru   \n",
       "3  Kolkata, Pune, Chennai, Bangalore/Bengaluru, D...   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                        company_name experience_required  \n",
       "0                 Inflexion Analytix Private Limited             0-3 Yrs  \n",
       "1                Shell India Markets Private Limited             6-9 Yrs  \n",
       "2  INTERTRUSTVITEOS CORPORATE AND FUND SERVICES P...           12-18 Yrs  \n",
       "3                     SA Tech Software (I) Pvt. Ltd.             1-3 Yrs  \n",
       "4   PHARMACEUTICAL RESEARCH ASSOCIATES INDIA Pvt Ltd             2-4 Yrs  \n",
       "5               Innovsource Services Private Limited             2-7 Yrs  \n",
       "6                H and M Hennes and Mauritz (P) Ltd.             4-7 Yrs  \n",
       "7                                             Luxoft             3-6 Yrs  \n",
       "8                              Philips India Limited            5-10 Yrs  \n",
       "9                  Flipkart Internet Private Limited             1-3 Yrs  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "except NoSuchElementException as e:\n",
    "    title_tags=' '\n",
    "\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "\n",
    "try:\n",
    "    location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    location_tags=' '\n",
    "    \n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "    \n",
    "try:\n",
    "    subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "except NoSuchElementException as e:\n",
    "    subtitle_tags=' '\n",
    "\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "try:\n",
    "    experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    experience_tags=' '\n",
    "    \n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "except NoSuchElementException as e:\n",
    "    title_tags=' '\n",
    "\n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "    \n",
    "try:\n",
    "    location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    location_tags=' '\n",
    "    \n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "try:\n",
    "    subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "except NoSuchElementException as e:\n",
    "    subtitle_tags=' '\n",
    "\n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "try:\n",
    "    experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    experience_tags=' '\n",
    "\n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Please note that you have to scrape full job description. For that you may have to open each job separately as shown below\n",
    "You will get the search result as shown in the below webpage:\n",
    "You have to click on each job title. Here in this case if you will click on “ Data Scientist, Machine Learning” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/data-scientist-jobs-in-banglore-bengaluru'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.naukri.com/job-listings-data-scientist-data-analyst-business-analyst-inflexion-analytix-private-limited-mumbai-hyderabad-secunderabad-pune-gurgaon-gurugram-chennai-bangalore-bengaluru-0-to-3-years-100521000368?src=jobsearchDesk&sid=16216767765869111&xp=1&px=1',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-wipro-limited-kolkata-hyderabad-secunderabad-chennai-bangalore-bengaluru-4-to-9-years-210521000038?src=jobsearchDesk&sid=16216767765869111&xp=2&px=1',\n",
       " 'https://www.naukri.com/job-listings-big-data-data-scientist-xoriant-solutions-pvt-ltd-kochi-cochin-indore-hyderabad-secunderabad-pune-ahmedabad-bangalore-bengaluru-mumbai-all-areas-1-to-3-years-130521005018?src=jobsearchDesk&sid=16216767765869111&xp=3&px=1',\n",
       " 'https://www.naukri.com/job-listings-specialist-i-data-scientist-philips-india-limited-bangalore-bengaluru-4-to-7-years-170521500993?src=jobsearchDesk&sid=16216767765869111&xp=4&px=1',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-ibm-india-pvt-limited-bangalore-bengaluru-6-to-8-years-110521907352?src=jobsearchDesk&sid=16216767765869111&xp=5&px=1',\n",
       " 'https://www.naukri.com/job-listings-lead-data-scientist-intel-technology-india-pvt-ltd-bangalore-bengaluru-6-to-10-years-070521500578?src=jobsearchDesk&sid=16216767765869111&xp=6&px=1',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-oracle-india-pvt-ltd-bangalore-bengaluru-6-to-10-years-190521008276?src=jobsearchDesk&sid=16216767765869111&xp=7&px=1',\n",
       " 'https://www.naukri.com/job-listings-sde-lead-data-scientist-l3-huawei-technologies-india-pvt-ltd-bangalore-bengaluru-5-to-8-years-120521901434?src=jobsearchDesk&sid=16216767765869111&xp=8&px=1',\n",
       " 'https://www.naukri.com/job-listings-computational-design-lead-data-scientist-l3-huawei-technologies-india-pvt-ltd-bangalore-bengaluru-5-to-8-years-120521901329?src=jobsearchDesk&sid=16216767765869111&xp=9&px=1',\n",
       " 'https://www.naukri.com/job-listings-hiring-for-data-scientist-on-contract-basis-3-6-months-globaledx-learning-and-technology-solution-pvt-ltd-hyderabad-secunderabad-bangalore-bengaluru-mumbai-all-areas-3-to-8-years-170521003581?src=jobsearchDesk&sid=16216767765869111&xp=10&px=1',\n",
       " 'https://www.naukri.com/job-listings-senior-data-scientist-go-jek-india-bangalore-bengaluru-4-to-11-years-190521500736?src=jobsearchDesk&sid=16216767765869111&xp=11&px=1',\n",
       " 'https://www.naukri.com/job-listings-senior-data-scientist-nanobi-data-and-analytics-private-limited-bangalore-bengaluru-7-to-9-years-190521501055?src=jobsearchDesk&sid=16216767765869111&xp=12&px=1',\n",
       " 'https://www.naukri.com/job-listings-sr-data-scientist-valiance-analytics-private-limited-bangalore-bengaluru-delhi-ncr-6-to-8-years-180521006944?src=jobsearchDesk&sid=16216767765869111&xp=13&px=1',\n",
       " 'https://www.naukri.com/job-listings-senior-data-scientist-bankbazaar-com-a-a-dukaan-financial-services-pvt-ltd-bangalore-bengaluru-5-to-7-years-040321006895?src=jobsearchDesk&sid=16216767765869111&xp=14&px=1',\n",
       " 'https://www.naukri.com/job-listings-senior-data-scientist-kwalee-bangalore-bengaluru-5-to-9-years-220521500306?src=jobsearchDesk&sid=16216767765869111&xp=15&px=1',\n",
       " 'https://www.naukri.com/job-listings-senior-associate-team-lead-data-scientist-consulting-analytics-india-magazine-bangalore-bengaluru-3-to-5-years-200521900965?src=jobsearchDesk&sid=16216767765869111&xp=16&px=1',\n",
       " 'https://www.naukri.com/job-listings-senior-data-scientist-gojek-tech-bangalore-bengaluru-3-to-7-years-200521500197?src=jobsearchDesk&sid=16216767765869111&xp=17&px=1',\n",
       " 'https://www.naukri.com/job-listings-data-scientist-applied-materials-india-private-limited-bangalore-bengaluru-2-to-4-years-070521901521?src=jobsearchDesk&sid=16216767765869111&xp=18&px=1',\n",
       " 'https://www.naukri.com/job-listings-sr-data-scientist-payments-airbnb-global-capability-center-private-limited-bangalore-bengaluru-4-to-9-years-210521900292?src=jobsearchDesk&sid=16216767765869111&xp=19&px=1',\n",
       " 'https://www.naukri.com/job-listings-hiring-for-lead-data-scientist-for-bangalore-location-societe-generale-global-solution-centre-pvt-ltd-bangalore-bengaluru-5-to-9-years-130521000901?src=jobsearchDesk&sid=16216767765869111&xp=20&px=1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "job_description=[]\n",
    "\n",
    "for i in urls[:10]:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        job=driver.find_element_by_xpath(\"//h1[@class='jd-header-title']\")\n",
    "        job_title.append(job.text)\n",
    "    except NoSuchElementException as e:\n",
    "        job_title.append('-')\n",
    "        \n",
    "    try:\n",
    "        desc=driver.find_element_by_xpath(\"//section[@class='job-desc']\")\n",
    "        job_description.append(desc.text)\n",
    "    except NoSuchElementException as e:\n",
    "        job_description.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title))\n",
    "print(len(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist / Data Analyst -Business Analyst',\n",
       " 'Data Scientist',\n",
       " '-',\n",
       " 'Specialist I - Data Scientist',\n",
       " '-',\n",
       " 'Lead Data Scientist',\n",
       " '-',\n",
       " 'SDE Lead Data Scientist-L3',\n",
       " 'Computational Design Lead Data Scientist-L3',\n",
       " 'Hiring For DATA Scientist - ON Contract Basis (3-6 Months)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "import selenium.webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.naukri.com/'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()\n",
    "\n",
    "loc=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[2]/div[2]/div[2]/label/p\")\n",
    "loc.click()\n",
    "Sal=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[2]/label/p\")\n",
    "Sal.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "except NoSuchElementException as e:\n",
    "    title_tags=' '\n",
    "    \n",
    "job_title=[]\n",
    "for i in title_tags:\n",
    "    job_title.append(i.text)\n",
    "\n",
    "try:\n",
    "    location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    location_tags=' '\n",
    "    \n",
    "job_location=[]\n",
    "for i in location_tags:\n",
    "    job_location.append(i.text)\n",
    "\n",
    "try:\n",
    "    subtitle_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "except NoSuchElementException as e:\n",
    "    subtitle_tags=' '\n",
    "    \n",
    "job_subtitle=[]\n",
    "for i in subtitle_tags:\n",
    "    job_subtitle.append(i.text)\n",
    "\n",
    "try:\n",
    "    experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    experience_tags=' '\n",
    "    \n",
    "job_experience=[]\n",
    "for i in experience_tags:\n",
    "    job_experience.append(i.text)\n",
    "\n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job-title']=job_title[0:10]\n",
    "jobs['job-location']=job_location[0:10]\n",
    "jobs['company_name']=job_subtitle[0:10]\n",
    "jobs['experience_required']=job_experience[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/index.htm'\n",
    "driver.get(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('sc.keyword')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('sc.location')\n",
    "search_loc.send_keys(\"Noida (India)\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr']\")\n",
    "search_btn.click()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    companyname_tags=driver.find_elements_by_xpath(\"//a[@class=' css-l2wjgv e1n63ojh0 jobLink']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    companyname_tags=' '\n",
    "    \n",
    "company_names=[]\n",
    "for i in companyname_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "try:\n",
    "    jobposteddays_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex align-items-end pl-std css-mi55ob']\")\n",
    "except NoSuchElementException as e:\n",
    "    jobposteddays_tags=' '\n",
    "    \n",
    "JobPosted_Days=[]\n",
    "for i in jobposteddays_tags:\n",
    "    JobPosted_Days.append(i.text)\n",
    "    \n",
    "try:\n",
    "    rating_tags=driver.find_elements_by_xpath(\"//span[@class='css-19pjha7 e1cjmv6j1']\")\n",
    "except NoSuchElementException as e:\n",
    "    rating_tags=' '\n",
    "    \n",
    "Company_Rating=[]\n",
    "for i in rating_tags:\n",
    "    Company_Rating.append(i.text)\n",
    "    \n",
    "    \n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['company name']=company_names[0:10]\n",
    "jobs['job posted days']=JobPosted_Days[0:10]\n",
    "jobs['company rating']=Company_Rating[0:10]\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_job=driver.find_element_by_id('KeywordSearch')\n",
    "search_job.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_loc=driver.find_element_by_id('LocationSearch')\n",
    "search_loc.send_keys(\"Noida (India)\")\n",
    "\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='gd-btn-mkt']\")\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    companyname_tags=driver.find_elements_by_xpath(\"//h3[@class='m-0 css-g261rn']//a\")\n",
    "except NoSuchElementException as e:\n",
    "    companyname_tags=' '\n",
    "    \n",
    "company_names=[]\n",
    "for i in companyname_tags:\n",
    "    company_names.append(i.text)\n",
    "    \n",
    "try:\n",
    "    AvgSalary_tags=driver.find_elements_by_xpath(\"//div[@class='col-12 col-lg-4 px-lg-0 d-flex align-items-baseline']//h3\")\n",
    "except NoSuchElementException as e:\n",
    "    AvgSalary_tags=' '\n",
    "    \n",
    "AvgSalary=[]\n",
    "for i in AvgSalary_tags:\n",
    "    AvgSalary.append(i.text)\n",
    "    \n",
    "try:\n",
    "    minsalary_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p\")\n",
    "except NoSuchElementException as e:\n",
    "    minsalary_tags=' '\n",
    "    \n",
    "MinSalary=[]\n",
    "for i in minsalary_tags:\n",
    "    MinSalary.append(i.text)\n",
    "MinSalary[0:20:2]\n",
    "\n",
    "try:\n",
    "    maxsalary_tags=driver.find_elements_by_xpath(\"//div[@class='d-flex mt-xxsm css-79elbk epuxyqn0']//p\")\n",
    "except NoSuchElementException as e:\n",
    "    maxsalary_tags=' '\n",
    "    \n",
    "MaxSalary=[]\n",
    "for i in maxsalary_tags:\n",
    "    MaxSalary.append(i.text)\n",
    "MaxSalary[1:21:2]\n",
    "\n",
    "try:\n",
    "    rating_tags=driver.find_elements_by_xpath(\"//span[@class='m-0 css-kyx745']\")\n",
    "except NoSuchElementException as e:\n",
    "    rating_tags=' '\n",
    "    \n",
    "CompanyRating=[]\n",
    "for i in rating_tags:\n",
    "    CompanyRating.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "Salaries=pd.DataFrame({})\n",
    "Salaries['company name']=company_names[0:10]\n",
    "Salaries['Average Salary']=AvgSalary[0:10]\n",
    "Salaries['Min Salary']=MinSalary[0:10]\n",
    "Salaries['Max Salary']=MaxSalary[0:10]\n",
    "Salaries['Company Rating']=CompanyRating[0:10]\n",
    "Salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "login_pop.click()\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sunglasses\")\n",
    "search_job.submit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "    \n",
    "Brand_name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_name.append(i.text)\n",
    "    \n",
    "try:\n",
    "    description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "except NoSuchElementException as e:\n",
    "    description_tags=' '\n",
    "    \n",
    "Product_Description=[]\n",
    "for i in description_tags:\n",
    "    Product_Description.append(i.text)\n",
    "    \n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "    \n",
    "Product_Price=[]\n",
    "for i in price_tags:\n",
    "    Product_Price.append(i.text)\n",
    "    \n",
    "try:\n",
    "    discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    discount_tags=' '\n",
    "    \n",
    "Price_Discount=[]\n",
    "for i in discount_tags:\n",
    "    Price_Discount.append(i.text)\n",
    "    \n",
    "import pandas as pd\n",
    "Sunglasses=pd.DataFrame({})\n",
    "Sunglasses['Brand']=Brand_name[0:100]\n",
    "Sunglasses['Description']=Product_Description[0:100]\n",
    "Sunglasses['Price']=Product_Price[0:100]\n",
    "Sunglasses['Discount']=Price_Discount[0:100]\n",
    "Sunglasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Best Phone for the Money\\n\\nThe iPhone 11 offers superb cameras, a more durable design and excellent battery life for an affordable price.\\n\\nCompelling ultra-wide camera\\nNew Night mode is excellent\\nLong battery life',\n",
       " 'Amazing phone with great cameras and better battery which gives you the best performance. I just love the camera .',\n",
       " 'Amazing Powerful and Durable Gadget.\\n\\nI’m am very happy with the camera picture quality, Amazing face id unlocked in dark room, Strong battery with perfect screen size as you can carry easily in pocket. This is my third iPhone.\\n\\nI shifted from android Samsung Note series to iPhone because of the strong build quality and peace of mind for next 3-4 years.\\n\\nDon’t think to much just go for it and I suggest you to go for minimum 128gb variant or more 256gb.\\n\\nI’ve attached my puppy pics and no fi...',\n",
       " 'Previously I was using one plus 3t it was a great phone\\nAnd then I decided to upgrade I am stuck between Samsung s10 plus or iPhone 11\\nI have seen the specs and everything were good except the display it’s somewhere between 720-1080 and it’s not even an amoled it’s an LCD display\\nBut I decided to go with iPhone because I have never used an IOS device I have Been an android user from the past 9 years I ordered IPhone 11 (128gb) product red\\nMy experience after using 3 weeks\\n1. The delivery ...',\n",
       " 'So far it’s been an AMAZING experience coming back to iOS after nearly a decade but it’s not as versatile as android though phone is sturdy dropped it accidentally a couple of times and nothing happened fortunately camera is awesome',\n",
       " 'iphone 11 is a very good phone to buy only if you can compromise for the display. The display on this is device is pretty good but you can get other options with better displays in this price segment.\\nIf you can survive with an HD+ LCD panel with thicker bezels and a notch up top then this is a very good phone for you.\\nCameras are awesome, battery backup excellent, great performance and a decent premium look. Good job Apple !',\n",
       " 'It’s a must buy who is looking for an upgrade from previous generation of iPhones. If you are using XR then still you can hold on for sometime and upgrade to 2020 model else this phone is a must buy . Camera quality is amazing and wide angle is something to count upon. Performance wise it’s amazing and feels premium while holding in hand. So a big YES for this device. Go for 128 GB variant as the 4K videos will occupy lots of space and the storage can get over very quickly. Try to buy it with...',\n",
       " 'This is my first iOS phone. I am very happy with this product. Very much satisfied with this. I love this phone.',\n",
       " 'Value for money❤️❤️\\nIts awesome mobile phone in the world ...\\nDisplay was very good and bright ..\\nTrust me freinds you r never regret after Buying..\\nJust go for it....\\nI love this phone and i switch to iphone x to 11',\n",
       " '*Review after 10 months of usage*\\nDoesn\\'t seem bulky with decent touch and camera. The \"standard maintained\" quality of IPhone. Go for it, if your budget is not more enough for pro models. It\\'s a value for money among the 11 series Stereo speakers are so good, you will love the stereo sound. Battery backup is not decent but still better than Iphone 7 and 8.Processor is fast, the phone doesn\\'t lag at all even with heavy games, waterproof, faster face id and with no fingerprint scanners. The ca...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "  \n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART'    \n",
    "    driver.get(url)\n",
    "    try:\n",
    "        rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "    except NoSuchElementException as e:\n",
    "        rating_tags='-'\n",
    "    \n",
    "    try:\n",
    "        summary_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "    except NoSuchElementException as e:\n",
    "        summary_tags='-'\n",
    "        \n",
    "    try:\n",
    "        FullReview_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']//div//div\")\n",
    "    except NoSuchElementException as e:\n",
    "        FullReview_tags='-'\n",
    "\n",
    "for i in range(len(rating_tags)):\n",
    "    element_list.append([rating_tags[i].text, summary_tags[i].text, FullReview_tags[i].text])\n",
    "\n",
    "\n",
    "with xlsxwriter.Workbook('IphoneReviewResult.xlsx') as workbook:\n",
    "    worksheet = workbook.add_worksheet()\n",
    "  \n",
    "for row_num, data in enumerate(element_list):\n",
    "    worksheet.write_row(row_num, 0, data)  \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "\n",
    "login_pop = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\")\n",
    "# Here .click function use to tap on desire elements of webpage\n",
    "login_pop.click()\n",
    "\n",
    "\n",
    "search_job=driver.find_element_by_xpath(\"//div[@class='_3OO5Xc']//input\")\n",
    "search_job.send_keys(\"sneakers\")\n",
    "search_job.submit()\n",
    "\n",
    "# for holding the resultant list\n",
    "element_list = []\n",
    "  \n",
    "for page in range(1, 11, 1):\n",
    "    driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "    url='https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off'    \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    except NoSuchElementException as e:\n",
    "        brand_tags=' '\n",
    "        \n",
    "    try:\n",
    "        ProductDesc_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    except NoSuchElementException as e:\n",
    "        ProductDesc_tags=' '\n",
    "        \n",
    "    try:\n",
    "        Price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "    except NoSuchElementException as e:\n",
    "        Price_tags=' '\n",
    "        \n",
    "    try:\n",
    "        Discount_tags=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']//span\")\n",
    "    except NoSuchElementException as e:\n",
    "        Discount_tags=' '\n",
    "        \n",
    "    for i in range(len(rating_tags)):\n",
    "        element_list.append([brand_tags[i].text, ProductDesc_tags[i].text, Price_tags[i].text, Discount_tags[i].text])\n",
    "  \n",
    "with xlsxwriter.Workbook('SneakersResult.xlsx') as workbook:\n",
    "    worksheet = workbook.add_worksheet()\n",
    "  \n",
    "    for row_num, data in enumerate(element_list):\n",
    "        worksheet.write_row(row_num, 0, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price filter to “Rs. 6649 to Rs. 13099”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label\")\n",
    "price.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=Brand_Name[0:10]\n",
    "shoes['Shoe Description']=Product_Desc[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Color filter to “Black”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.myntra.com/shoes'\n",
    "driver.get(url)\n",
    "\n",
    "color=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label\")\n",
    "color.click()\n",
    "\n",
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']//span\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes['Brand']=job_title[0:10]\n",
    "shoes['Shoe Description']=job_location[0:10]\n",
    "shoes['Price']=Prices[0:10]\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import xlsxwriter\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_loc.send_keys(\"Laptop\")\n",
    "\n",
    "search_btn=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_btn.click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu1=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[9]/span/a/span\")\n",
    "cpu1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[1]/div/span[3]/div[2]/div[2]/div/span/div/div/div[2]/div[2]/div/div[2]/div/span[1]/span/a/i[1]/span\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "        \n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "        \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['Title']=Brand_Name[0:10]\n",
    "laptop['Rating']=Product_Desc[0:10]\n",
    "laptop['Price']=Prices[0:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set filter “Intel Core i9”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import xlsxwriter\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\sagar\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "\n",
    "url='https://www.amazon.in/'\n",
    "driver.get(url)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc=driver.find_element_by_id('twotabsearchtextbox')\n",
    "search_loc.send_keys(\"Laptop\")\n",
    "\n",
    "search_btn=driver.find_element_by_id('nav-search-submit-button')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu2=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[8]/span/a/div/label/span\")\n",
    "cpu2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    brand_tags=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "except NoSuchElementException as e:\n",
    "    brand_tags=' '\n",
    "        \n",
    "Brand_Name=[]\n",
    "for i in brand_tags:\n",
    "    Brand_Name.append(i.text)\n",
    "\n",
    "try:\n",
    "    desc_tags=driver.find_elements_by_xpath(\"/html/body/div[1]/div[2]/div[1]/div/div[1]/div/span[3]/div[2]/div[2]/div/span/div/div/div[2]/div[2]/div/div[2]/div/span[1]/span/a/i[1]/span\")\n",
    "except NoSuchElementException as e:\n",
    "    desc_tags=' '\n",
    "Product_Desc=[]\n",
    "for i in desc_tags:\n",
    "    Product_Desc.append(i.text)\n",
    "\n",
    "try:\n",
    "    price_tags=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "except NoSuchElementException as e:\n",
    "    price_tags=' '\n",
    "    \n",
    "Prices=[]\n",
    "for i in price_tags:\n",
    "    Prices.append(i.text)\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop['Title']=Brand_Name[0:10]\n",
    "laptop['Rating']=Product_Desc[0:10]\n",
    "laptop['Price']=Prices[0:10]\n",
    "laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
